{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJni_Fa-JSXj"
   },
   "source": [
    "# Run this to load libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77744,
     "status": "ok",
     "timestamp": 1602101578548,
     "user": {
      "displayName": "Louis McCallum",
      "photoUrl": "",
      "userId": "16483688816273934191"
     },
     "user_tz": -60
    },
    "id": "kBKGosNv2Gti",
    "outputId": "bdddafb4-9ceb-4ae8-b523-8e13b08c5abc"
   },
   "outputs": [],
   "source": [
    "!pip install -qU ddsp[data_preparation]\n",
    "\n",
    "# Initialize global path for using google drive. \n",
    "DRIVE_DIR = ''\n",
    "import os\n",
    "import ddsp\n",
    "import ddsp.training\n",
    "import gin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io.wavfile as wavutils\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in /home/louis/anaconda3/envs/ddsp/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /home/louis/.local/lib/python3.6/site-packages (from sounddevice) (1.14.3)\n",
      "Requirement already satisfied: pycparser in /home/louis/.local/lib/python3.6/site-packages (from CFFI>=1.0->sounddevice) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/home/louis/anaconda3/envs/ddsp/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install sounddevice --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sounddevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1b8d9fe8fdfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vXbMKLuJtTa"
   },
   "source": [
    "# Define custom methods (just run the cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2370,
     "status": "ok",
     "timestamp": 1602105763095,
     "user": {
      "displayName": "Louis McCallum",
      "photoUrl": "",
      "userId": "16483688816273934191"
     },
     "user_tz": -60
    },
    "id": "BnVH2YI23Pdn"
   },
   "outputs": [],
   "source": [
    "class UnitProvider():\n",
    "    def __init__(self):\n",
    "        self.unit_list = []\n",
    "        self.units = 1;\n",
    "\n",
    "    def get_units(self, s):\n",
    "        if len(self.unit_list) == 0:\n",
    "            self.unit_list = np.arange(s)\n",
    "            np.random.shuffle(self.unit_list)\n",
    "            self.unit_list = self.unit_list[:int(s * self.units)]\n",
    "        #print(len(self.unit_list))\n",
    "        return self.unit_list\n",
    "\n",
    "class BendingParam():\n",
    "    def __init__(self):\n",
    "        self.t = 0\n",
    "        #number of vals in block\n",
    "        self.res = 1000\n",
    "        self.unit_list = []\n",
    "        self.lfo = False\n",
    "        self.ramp = False\n",
    "        self.scalar = 0\n",
    "        self.min = 0\n",
    "        self.max = 1\n",
    "        self.freq = 1\n",
    "        self.len = 1\n",
    "    \n",
    "    #return 1 block of params\n",
    "    def get_values(self):\n",
    "        vals = []\n",
    "        if self.lfo:\n",
    "          r = (self.max - self.min) / 2\n",
    "          vals = np.array([self.step_lfo() for i in range(self.res)])\n",
    "          vals = vals + (1 + self.min)\n",
    "          vals = vals * r\n",
    "        elif self.ramp:\n",
    "          vals = np.linspace(self.min, self.max, self.len * self.res)[self.t:self.t+self.res]\n",
    "          self.t = self.t + self.res\n",
    "        else:\n",
    "          vals = np.ones(self.res) * self.scalar\n",
    "        return vals\n",
    " \n",
    "    def step_lfo(self):\n",
    "        increment = (self.freq / self.res) * (np.pi * 2)\n",
    "        val = np.sin(self.t)\n",
    "        self.t = self.t + increment\n",
    "        return val\n",
    "        \n",
    "class BendingTransforms():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t = 0\n",
    "        self.res = 1000\n",
    "        \n",
    "    def ablate(self, src, units):\n",
    "        src = src.numpy()\n",
    "        src = src.reshape((src.shape[1], src.shape[2]))\n",
    "        M, N = src.shape\n",
    "        units = units.get_units(N)\n",
    "        src[:,units] = 0\n",
    "        return src.reshape((1, M, N))\n",
    "    \n",
    "    def invert(self, src, units):\n",
    "        src = src.numpy()\n",
    "        src = src.reshape((src.shape[1], src.shape[2]))\n",
    "        M, N = src.shape\n",
    "        units = units.get_units(N)\n",
    "        src[:,units] = 1 - src[:,units]\n",
    "        return src.reshape((1, M, N))\n",
    "    \n",
    "    def threshold(self, src, thresh, units):\n",
    "        thresh = thresh.get_values()\n",
    "        #apply in axis 1 (time)\n",
    "        thresh = thresh.reshape((thresh.shape[0], 1))\n",
    "        src = src.numpy()\n",
    "        one, M, N = src.shape\n",
    "        src = src.reshape((M, N))\n",
    "        units = units.get_units(N)\n",
    "        #print(src[src < t], t, src)\n",
    "        src[:,units][src[:,units] < thresh] = 0\n",
    "        src[:,units][src[:,units] >= thresh] = 1\n",
    "        return src.reshape((1, M, N))\n",
    "                    \n",
    "    def step_osc(self, f = 1.0):\n",
    "        increment = (f / self.res) * (np.pi * 2)\n",
    "        self.t = self.t + increment\n",
    "        return np.sin(self.t)\n",
    "    \n",
    "    def oscillate(self, src, freq, depth, units):\n",
    "        src = src.numpy()\n",
    "        src = src.reshape((src.shape[1], src.shape[2]))\n",
    "        M, N = src.shape\n",
    "        f = freq.get_values()\n",
    "        d = depth.get_values()\n",
    "        b = np.array([self.step_osc(f[i]) for i in range(0,self.res)]) * d\n",
    "        #apply in axis 1 (time)\n",
    "        b = b.reshape(b.shape[0], 1)\n",
    "        units = units.get_units(N)\n",
    "        src[:,units] = src[:,units] + b\n",
    "        return src.reshape((1, M, N))\n",
    "\n",
    "    def reflect(self, src, r, units):\n",
    "        alpha = r\n",
    "        a = np.array([[np.cos(2*alpha), np.sin(2*alpha)],\n",
    "                      [np.sin(2*alpha), -np.cos(2*alpha)]])\n",
    "        return self.linear_transformation(src, a)\n",
    "    \n",
    "    def rotate(self, src, radians, units):\n",
    "        alpha = radians\n",
    "        a = np.array([[np.cos(alpha), -np.sin(alpha)],\n",
    "                      [np.sin(alpha), np.cos(alpha)]])\n",
    "        return self.linear_transformation(src, a)\n",
    "    \n",
    "    def linear_transformation(self, src, a):\n",
    "        src = src.numpy()\n",
    "        src = src.reshape((src.shape[1], src.shape[2]))\n",
    "        M, N = src.shape\n",
    "        points = np.mgrid[0:N, 0:M].reshape((2, M*N))\n",
    "        new_points = np.linalg.inv(a).dot(points).round().astype(int)\n",
    "        x, y = new_points.reshape((2, M, N), order='F')\n",
    "        indices = x + N*y\n",
    "        wrap = np.take(src, indices, mode='wrap').reshape((1, M, N))\n",
    "        t = tf.constant(wrap)\n",
    "        return t\n",
    "\n",
    "class BendingDecoder(ddsp.training.decoders.RnnFcDecoder):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"bending init called\")\n",
    "\n",
    "    def init_params(self):\n",
    "        print(\"BendingDecoder init_params\")\n",
    "        self.t = {}\n",
    "        self.t[\"FC1\"] = []\n",
    "        self.t[\"FC2\"] = []\n",
    "        self.t[\"GRU\"] = []\n",
    "        \n",
    "    def add_transform(self, layer, f, a):\n",
    "        print(\"adding transform\", layer, f, a)\n",
    "        self.t[layer].append(tf.keras.layers.Lambda(f, arguments = a))\n",
    "\n",
    "    def compute_output(self, *inputs):\n",
    "      # Initial processing.\n",
    "      print(\"BendingDecoder compute_output\")\n",
    "      inputs = [stack(x) for stack, x in zip(self.input_stacks, inputs)]\n",
    "\n",
    "      # Run an RNN over the latents.\n",
    "      x = tf.concat(inputs, axis=-1)\n",
    "      for f in self.t[\"FC1\"]:\n",
    "            x = f(x)\n",
    "      x = self.rnn(x)\n",
    "      for f in self.t[\"GRU\"]:\n",
    "            x = f(x)\n",
    "      x = tf.concat(inputs + [x], axis=-1)\n",
    "\n",
    "      # Final processing.\n",
    "      x = self.out_stack(x)\n",
    "      for f in self.t[\"FC2\"]:\n",
    "            print(\"calling FC2\", f)\n",
    "            x = f(x)\n",
    "      return x\n",
    "\n",
    "class Generator():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = [\"FC1\", \"GRU\", \"FC2\"]\n",
    "        self.transforms = {}\n",
    "        self.buf_length = 16000\n",
    "        for l in self.layers:\n",
    "            self.transforms[l] = BendingTransforms()\n",
    "    \n",
    "    # setup tensorflow, the feature extractor and the model\n",
    "    def setup_resynthesis(self, model_dir):\n",
    "        \"\"\"\n",
    "        initialisesm the resynthesis models\n",
    "        and reset the crepe feature extractor\n",
    "        \"\"\"\n",
    "        #self.setup_tensorflow()\n",
    "        ddsp.spectral_ops.reset_crepe()\n",
    "        self.setup_model(model_dir)\n",
    "        print(\"setup_resynthesis::resynthesis ready probably\")\n",
    "        self.model.decoder.__class__ = BendingDecoder\n",
    "        self.model.decoder.init_params()\n",
    "    \n",
    "    def setup_tensorflow(self):\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        session = tf.compat.v1.Session(config=config)\n",
    "        tf.compat.v1.keras.backend.set_session(session)\n",
    "        print(\"setup_tensorflow\")\n",
    "        \n",
    "    def setup_model(self, model_dir):\n",
    "        gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
    "\n",
    "        if os.path.isfile(gin_file) != True:\n",
    "            print(\"setup_model::Gin file not found: \", gin_file)\n",
    "            return \n",
    "\n",
    "         # Parse gin config,\n",
    "        with gin.unlock_config():\n",
    "            gin.parse_config_file(gin_file, skip_unknown=True)\n",
    "\n",
    "        # Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
    "        ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
    "        ckpt_name = ckpt_files[0].split('.')[0]\n",
    "        ckpt = os.path.join(model_dir, ckpt_name)\n",
    "\n",
    "        # Ensure dimensions and sampling rates are equal\n",
    "        #time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
    "        time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
    "        #n_samples_train = gin.query_parameter('Additive.n_samples')\n",
    "        n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
    "        hop_size = int(n_samples_train / time_steps_train)\n",
    "\n",
    "        time_steps = int(self.buf_length / hop_size)\n",
    "        required_input_samples = time_steps * hop_size\n",
    "        print(\"time steps\", time_steps, time_steps_train)\n",
    "        print(\"input_samples\", required_input_samples, n_samples_train)\n",
    "\n",
    "        gin_params = [\n",
    "            'RnnFcDecoder.input_keys = (\"f0_scaled\", \"ld_scaled\", \"z\")',\n",
    "            'Additive.n_samples = {}'.format(required_input_samples),\n",
    "            'FilteredNoise.n_samples = {}'.format(required_input_samples),\n",
    "            'DefaultPreprocessor.time_steps = {}'.format(time_steps),\n",
    "        ]\n",
    "\n",
    "        # with gin.unlock_config():\n",
    "        #     gin.parse_config(gin_params)\n",
    "\n",
    "        # Set up the model just to predict audio given new conditioning\n",
    "        self.model = ddsp.training.models.Autoencoder()\n",
    "        self.model.restore(ckpt) \n",
    "        # gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
    "        # gin.parse_config_file(gin_file)\n",
    "        # self.model = ddsp.training.models.Autoencoder()\n",
    "        # self.model.restore(model_dir)\n",
    "    \n",
    "    def resynth_batch(self, data_dir):\n",
    "        TRAIN_TFRECORD = data_dir + '/train.tfrecord'\n",
    "        TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
    "        data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "        dataset = data_provider.get_batch(batch_size=1, shuffle=False)\n",
    "\n",
    "        try:\n",
    "          batch = next(iter(dataset))\n",
    "        except OutOfRangeError:\n",
    "          raise ValueError(\n",
    "              'TFRecord contains no examples. Please try re-running the pipeline with '\n",
    "              'different audio file(s).')\n",
    "        print(batch[\"f0_hz\"].shape)\n",
    "        audio_gen = self.model(batch, training=False)\n",
    "        return audio_gen, batch['audio']\n",
    "\n",
    "    @staticmethod\n",
    "    def load_audio_data(audio_filename):\n",
    "        \"\"\"\n",
    "        reads all samples from the senf audio_filename\n",
    "        returns a numpy array of the samples and the sample rate \n",
    "        \"\"\"\n",
    "        signal, sr=librosa.load(audio_filename, sr=16000, mono = True,)\n",
    "        print(\"loaded audio file\", len(signal))\n",
    "        return np.array(signal), sr\n",
    "        \n",
    "    def extract_features_and_write_to_file(self, audio_filename):\n",
    "        \"\"\"\n",
    "        looks for a file called audio_filename.csv\n",
    "        if it does not exist, extracts features\n",
    "        using the function ddsp.training.metrics.compute_audio_features\n",
    "        and writes them to that file\n",
    "        returns the csv filename\n",
    "        \"\"\"\n",
    "        audio_signal, sr = self.load_audio_data(audio_filename)\n",
    "        feature_filaname = audio_filename + \".csv\"\n",
    "        if not os.path.exists(feature_filaname):\n",
    "            print(\"load_features::extracting features from \", audio_filename, ' (slow on CPU!)')\n",
    "            #audio_features = self.extract_audio_file_features(audio_sig, sr)\n",
    "            start_time = time.time()\n",
    "            print('Extracting features (may take a while). Sig length ', len(audio_signal))\n",
    "            audio_features = ddsp.training.metrics.compute_audio_features(audio_signal, sample_rate=sr)\n",
    "            print('extract_input_fetures:: Audio features took %.1f seconds' % (time.time() - start_time))\n",
    "            audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
    "            stacked = np.stack((audio_features[\"f0_hz\"], audio_features[\"loudness_db\"],audio_features[\"f0_confidence\"]), axis=1)\n",
    "            df = pd.DataFrame(stacked,columns=[\"f0_hz\",\"loudness_db\",\"f0_confidence\"])\n",
    "            df.to_csv(feature_filaname)\n",
    "        else:\n",
    "            print(\"features already extracted, found csv\") \n",
    "        \n",
    "        return feature_filaname\n",
    "    \n",
    "    def write_file(self, output, config = None, normalise = False, sample_rate = 16000):\n",
    "        complete_output = np.zeros((2, len(output)))\n",
    "        complete_output[0] = complete_output[1] = output\n",
    "\n",
    "        print(\"main:: synthesis ends...\" + str(len(output)))\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "        output_root = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "        output_audio_file = output_root + \".wav\"\n",
    "        output_json_file = output_root + \".json\"\n",
    "        boost_left = boost_right = 1\n",
    "        if normalise:\n",
    "          boost_left = self.get_normalise_scalar(complete_output[0])\n",
    "          boost_right = self.get_normalise_scalar(complete_output[1])\n",
    "        \n",
    "        if not config == None:\n",
    "          output_file = os.path.join(AUDIO_DATA_DIR, output_json_file);\n",
    "          print(\"writing config to json\", output_file)\n",
    "          with open(output_file, 'w') as outfile:\n",
    "              json.dump(config, outfile)\n",
    "\n",
    "        complete_output[0] = complete_output[0] * boost_left\n",
    "        complete_output[1] = complete_output[1] * boost_right\n",
    "\n",
    "        amplitude = np.iinfo(np.int16).max\n",
    "        complete_output = complete_output * amplitude\n",
    "        #now rotate it from [[ch1...], [ch2...] to [[c1, c2], [c1, c2] ..]\n",
    "        complete_output = np.rot90(complete_output, 3) # 3 as 1 is reversed\n",
    "        #complete_output = np.array([int((x + 1) * 32768) for x in complete_output])\n",
    "        output_path = os.path.join(AUDIO_DATA_DIR, output_audio_file);\n",
    "        wavutils.write(output_path, sample_rate, complete_output.astype(np.int16))\n",
    "\n",
    "        print(\"main:: wrote result to \", output_file)\n",
    "        \n",
    "    def get_normalise_scalar(self, buffer):\n",
    "        max = 0\n",
    "        for i in range(len(buffer)):\n",
    "            if np.abs(buffer[i]) > max:\n",
    "                max = np.abs(buffer[i])\n",
    "        scalar = 1/max\n",
    "        return scalar\n",
    "    \n",
    "\n",
    "\n",
    "    def combine_features_and_audio(self, csv_file, audio_file, samplerate = 16000, start = 0.0, end = 1.0):\n",
    "        \"\"\"\n",
    "        creates a basic data structure containing\n",
    "        features and audio signal \n",
    "        assumes the csv_file exists \n",
    "        more processing is needed before the data can be fed \n",
    "        to the model. That is done by load_and_prepare_features_for_model\n",
    "        which actually calls me\n",
    "        \"\"\"\n",
    "        #df = pd.read_csv(os.path.join(AUDIO_DATA_DIR,name + \".csv\"))\n",
    "        df = pd.read_csv(csv_file)\n",
    "        total = np.array(df[\"f0_hz\"]).shape[0]\n",
    "        start = int(total * start)\n",
    "        end = int(total * end)\n",
    "        print(\"loaded features from {s} to {e}\".format(s=start, e=end))\n",
    "        features = {}\n",
    "        features[\"f0_hz\"] = np.array(df[\"f0_hz\"])[start:end]\n",
    "        features[\"loudness_db\"] = np.array(df[\"loudness_db\"])[start:end]\n",
    "        features[\"f0_confidence\"] = np.array(df[\"f0_confidence\"])[start:end]\n",
    "        ## note that I don't think we need to add the original\n",
    "        ## audio signal to the features that are fed to the model\n",
    "        #features[\"audio\"] = audio_signal[start:end]\n",
    "        # audio_signal,samplerate = self.load_audio_data(audio_file)\n",
    "        # total = len(audio_signal)\n",
    "        # start = int(total * start)\n",
    "        # end = int(total * end)\n",
    "        # we do need the sample rate though\n",
    "        features[\"sr\"] = samplerate\n",
    "        return features\n",
    "    \n",
    "    def load_and_prepare_features_for_model(self, csv_file, audio_file, config, floor = True):    \n",
    "        \"\"\"\n",
    "        gets the input ready for the model\n",
    "        loads in the features and the signal\n",
    "        then prepares it in blocks \n",
    "        \"\"\"        \n",
    "        audio_features = self.combine_features_and_audio(\n",
    "          #config[\"features\"][\"file_name\"], \n",
    "          csv_file, \n",
    "          audio_file, \n",
    "          16000, \n",
    "          config[\"features\"][\"start\"],\n",
    "          config[\"features\"][\"end\"]\n",
    "        )\n",
    "        self.buf_length = config[\"input_buf_length\"]\n",
    "        self.frames = config[\"frames\"]\n",
    "        db_boost = config[\"db_boost\"]\n",
    "        r = np.floor if floor else np.ceil\n",
    "        steps = r(len(audio_features[\"f0_hz\"]) / self.frames )\n",
    "        def get_dict(start, af):\n",
    "            d = {}\n",
    "            f_start = int(start * self.frames )\n",
    "            s_start = int(start * self.buf_length)\n",
    "            d[\"f0_hz\"] = af[\"f0_hz\"][f_start:f_start+self.frames]\n",
    "            d[\"loudness_db\"] = af[\"loudness_db\"][f_start:f_start+self.frames ] + db_boost\n",
    "            d[\"f0_confidence\"] = af[\"f0_confidence\"][f_start:f_start+self.frames]\n",
    "            delta = self.frames - len(d[\"f0_hz\"])\n",
    "            if delta > 0:\n",
    "               d[\"f0_hz\"] = np.append(d[\"f0_hz\"], np.zeros(delta))\n",
    "               d[\"f0_confidence\"] = np.append(d[\"f0_confidence\"], np.zeros(delta))\n",
    "               d[\"loudness_db\"] = np.append(d[\"loudness_db\"], np.zeros(delta))\n",
    "            ## note I don't think we need to put the original\n",
    "            ## audio signal into the features that are fed into the model\n",
    "            #d[\"audio\"] = [af[\"audio\"][s_start:s_start+self.buf_length]]\n",
    "            return d\n",
    "\n",
    "        split = [get_dict(i, audio_features) for i in np.arange(steps)]\n",
    "        return np.array(split), steps\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_config(config):\n",
    "        \"\"\"\n",
    "        verify the sent config has the correct fields\n",
    "        uses assert so it will end execution if anything is missing\n",
    "        \"\"\"\n",
    "        want_keys = [\"features\", \"input_buf_length\", \"frames\", \"db_boost\", \"model_dir\", \"frames\"]\n",
    "        for key in want_keys:\n",
    "            assert key in config.keys(), \"missing config key \"+key\n",
    "            print(\"check_config::Config has key\", key)\n",
    "        print(\"check_config::Config looks good\")\n",
    "\n",
    "\n",
    "    def add_transforms(self, config, duration):\n",
    "        \"\"\"\n",
    "        adds the network bending transforms to the network\n",
    "        as specified by config\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "        #if transforms given for layer l\n",
    "            if l in config.keys():\n",
    "                c = config[l]\n",
    "                for f in c:\n",
    "                    arg = {}\n",
    "                    units = 1;\n",
    "                    if \"units\" in f.keys():\n",
    "                        units = f[\"units\"]\n",
    "                    arg[\"units\"] = UnitProvider()\n",
    "                    arg[\"units\"].units = units\n",
    "                    if \"params\" in f.keys():\n",
    "                        for p in f[\"params\"]:\n",
    "                            arg[p[\"name\"]] = BendingParam()\n",
    "                            arg[p[\"name\"]].res = self.frames\n",
    "                            arg[p[\"name\"]].len = int(np.ceil(duration))\n",
    "                            if \"args\" in p.keys():\n",
    "                                for k,v in p[\"args\"].items():\n",
    "                                    setattr(arg[p[\"name\"]], k, v)\n",
    "                    self.model.decoder.add_transform(l, getattr(self.transforms[l], f[\"function\"]), arg)\n",
    "\n",
    "\n",
    "    def run_features_through_model(self, audio_features):\n",
    "        \"\"\"\n",
    "        runs the sent features through the model one block at a time\n",
    "        and concatenates the result\n",
    "        returns an audio signal that is the result\n",
    "        \"\"\"\n",
    "        output = [self.run_feature_block_through_model(i) for i in audio_features]\n",
    "        faded = []\n",
    "        output = np.array(output).flatten()\n",
    "        return output\n",
    "    \n",
    "    def run_feature_block_through_model(self, ft):\n",
    "        \"\"\"\n",
    "        runs a single block of features through the model\n",
    "        returns and audio signal which is the result\n",
    "        \"\"\"\n",
    "        print(\"getting next block\")\n",
    "        outputs = self.model(ft, training=False)\n",
    "        audio = self.model.get_audio_from_outputs(outputs)\n",
    "        return audio\n",
    "\n",
    "\n",
    "    def resynthesize(self, feature_csv_filename, audio_filename, config):\n",
    "        \"\"\"\n",
    "        top level function that does resynthesis\n",
    "        it prepares the basic models, reads and prepares the features \n",
    "        adds transformations then calls\n",
    "        assumes that the features have already been extracted from\n",
    "        the input file (audio_filename)\n",
    "        \"\"\"\n",
    "        # setup the model\n",
    "        self.setup_resynthesis(config[\"model_dir\"]) \n",
    "        # get the features ready\n",
    "        audio_features, duration = self.load_and_prepare_features_for_model(feature_csv_filename, audio_filename, config)\n",
    "        # setup the bending transforms\n",
    "        for l in self.layers:\n",
    "            self.transforms[l].res = self.frames;\n",
    "        self.add_transforms(config, duration)\n",
    "        # resynthesize\n",
    "        output = self.run_features_through_model(audio_features)\n",
    "        print(\"DONE\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E80_1_hlJmar"
   },
   "source": [
    "# Specify Model folder\n",
    "We use the defauly Flute model model copied across from the github repo. If you want to use your own, you;ll need to mount your google drive and specify the path to somewhere on there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1602101778612,
     "user": {
      "displayName": "Louis McCallum",
      "photoUrl": "",
      "userId": "16483688816273934191"
     },
     "user_tz": -60
    },
    "id": "VrMwGgLi3jZ8"
   },
   "outputs": [],
   "source": [
    "model_name = \"Flute2021New\"\n",
    "DRIVE_DIR = '/content/network-bending'\n",
    "if DRIVE_DIR:\n",
    "  MODEL_DIR = os.path.join(DRIVE_DIR, 'Models/' + model_name)\n",
    "  AUDIO_DATA_DIR = os.path.join(DRIVE_DIR, 'audio_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vH1YxsnWJy5-"
   },
   "source": [
    "# Load audio features for input\n",
    "\n",
    "We use a choral piece for input by default, again copied across from the GitHub repo. \n",
    "\n",
    "If you want to use your own, you can mount your Google Drive and point to an audio file. It will only analyse audio the first time, will save as .CSV.\n",
    "\n",
    "Give the name of a .wav file in the AUDIO_DATA_DIR (\"audio_data\")\n",
    "\n",
    "Make sure GPU acceleration is on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUBBd7L9KK2c"
   },
   "source": [
    "# Specify transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17128,
     "status": "ok",
     "timestamp": 1602107030589,
     "user": {
      "displayName": "Louis McCallum",
      "photoUrl": "",
      "userId": "16483688816273934191"
     },
     "user_tz": -60
    },
    "id": "dfkV0Z_s5VIa",
    "outputId": "856ac61a-582f-4959-edf6-2f6068b623c5"
   },
   "outputs": [],
   "source": [
    "##See Instructions (https://github.com/Louismac/network-bending/blob/main/README.md)\n",
    "samplerate = 16000\n",
    "input_file = AUDIO_DATA_DIR + \"/monk-48.wav\"\n",
    "config = {}\n",
    "config[\"model_dir\"] = MODEL_DIR\n",
    "#pick how much of input file to do (0->1)\n",
    "config[\"features\"] = {\"file_name\":input_file, \"start\":0, \"end\":1}\n",
    "#add boost to loudness feature of input\n",
    "config[\"db_boost\"] = 10\n",
    "#4 secs at 16000\n",
    "config[\"input_buf_length\"] = 4 * samplerate\n",
    "config[\"frames\"] = 1000\n",
    "#transforms for first layer\n",
    "config[\"FC1\"] = [\n",
    " {\n",
    "    \"function\":\"ablate\",\n",
    "    \"units\":0.7,\n",
    "}\n",
    "]\n",
    "\n",
    "g = Generator()\n",
    "g.check_config(config)\n",
    "# step 1: write features to CSV file\n",
    "feature_csvfile = g.extract_features_and_write_to_file(input_file)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "# step 2: do the resynthesis\n",
    "audio_gen = g.resynthesize(feature_csvfile, input_file, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXG1HQaYKWo6"
   },
   "source": [
    "# Audition output\n",
    "We zero pad the end so thats what the noise is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "executionInfo": {
     "elapsed": 1859,
     "status": "ok",
     "timestamp": 1602107033199,
     "user": {
      "displayName": "Louis McCallum",
      "photoUrl": "",
      "userId": "16483688816273934191"
     },
     "user_tz": -60
    },
    "id": "rUoQ_hR58QGm",
    "outputId": "fa9ed8c9-03db-4504-d6ca-e15ebf8a95a7"
   },
   "outputs": [],
   "source": [
    "\n",
    "specplot(audio_gen)\n",
    "play(audio_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4WpX4dCJQta"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNfyhjLS9H2K5620GFoeo7T",
   "collapsed_sections": [
    "NJni_Fa-JSXj"
   ],
   "name": "NetworkBending.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
